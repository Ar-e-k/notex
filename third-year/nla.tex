\documentclass[12pt, letterpaper]{article}

\usepackage{graphicx}
\usepackage{parskip} % Disabling paragraph index as it does not fit maths
\usepackage{hyperref} % Usable menu and references
\usepackage{amssymb} % Used to show sets of sumbers, like the real numbers

\graphicspath{{images}}

\title{Natural Computing Algorithms }
\author{Arkadiusz Naks}
\date{2023}

\begin{document}

\begin{titlepage}
  \begin{center}
    \makeatletter
    \vspace*{1cm}
    \Huge
    \textbf{\@title}

    \vspace{0.5cm}
    \Large
    Lecture notes from Natural Computing Algorithms at Durham University

    \vspace{1.5cm}

    \textbf{\@author}

    % \includegraphics[scale=0.55]{.png}
    \vfill

    \vspace{0.8cm}

    \small
    Based on my understanding of lectures and notes of \\
    \@date{}
  \end{center}
\end{titlepage}

\tableofcontents
\newpage

\begin{section}{Important Definisions}

\end{section}

\begin{section}{Introduction}

\end{section}

\begin{section}{Immune system}

  \begin{subsection}{Negative Selection}

  \begin{subsubsection}{Base}

    This algorithm is based on the immune system's discrimination between
    \textbf{self} and \textbf{non-self}. Agents are generated by a random
    generation of genes. The agents which can recognise \textbf{self} are
    unwanted and therefor removed. This agents are equivelant to
    \textbf{T-cells} in human bodies.
    This algorithm was first used to identify computer viruses. It is usually
    applied for classification purposes. Its advantage is that it does not need
    any positive sample to identify the positives, only the negatives.

    Detector failure is defined if the detector is within a given theshold r
    from \textbf{self}. This r can be changed to adjust performace and accuracy
    of the algorithm. Another two variables that can be adjusted to impact the
    performance and accuracy are \textbf{self} sample size and the number of
    agents.

    \textit{This algorithm can also be used for symbol based datasets, with
    approprite notion of distance.}

  \end{subsubsection}

  \begin{subsubsection}{Rational Negative Selection}

    The name rational reflets on nothing. This is essentially a spin on the
    original negative selection with some influence of particle swarm. Rather
    than killing and regenerating all agents which detect \textbf{self}, they
    are moved around similarly to particle swarm and age is added. They are
    only deleted if they cannot find a \textbf{non-self} space before certain
    age.

  \end{subsubsection}

  \begin{subsubsection}{V-detectors}

    Essentially \textbf{rational negative selection} with detectors of variable
    size. This is usefull to cover small spaces without overlapping. While
    generating new detectors, \textbf{self} is initially ignored so the detector
    can be generated whenether there is sufficient space. Only then self is
    checked (\textbf{Phase 2}). In this phase a detector survives if it is at
    least \(r_{s}\) from any element in \textbf{self} (based on the traning set).
    Traning ends when \(t_{0} \geq \frac{1}{1 - c_{0}}\) with \(t_{0}\) being
    the number of iterations since last detector was added and \(c_{0}\) is
    the target coverage.

  \end{subsubsection}

  \end{subsection}

\end{section}

\begin{section}{Nature Inspired Algorithms}

  \begin{subsection}{Bees}

    This algorithm is inspired by how bee hives find and harvest food. There
    are 3 different type of agents, workers, onlookers and scouts. At the
    beggining each worker goes to a known food source and explores the
    neighbourhood of it for more \textbf{profitible} source. Secondly the
    workers communicate to the onlookers how good their foodsource is. This
    selection is done probabilistically, with probability proportional to
    \textbf{profitiblily} \( > 0\). Each onlooker also checks the
    neighbourhood. If the neighbourhood is checked multiple times without
    finding a better foodsource, the foodsoure is abandoned and the worker is
    reassigned to a new foodsource found by a scout. It is usefull not to
    discard the best foodsource even if it has no good neighbours.

    \begin{subsubsection}{Discrete}

      This version of the algorithm only works on continues cases, which means
      it is not applicable to many real world problems which are discrete, such
      as graph colouring and many other graph problems.

      Problem of random generation can be solved by using a greedy algorithm
      running in ranom order. Although this works, it limits the initial amount
      of chosing options and it can be much more computentionally expensive
      than just colour random numbers.

      Problem of random neighbour.

      Alternatively a problem can be left as continues and the further the
      values are from the discrete targets, the worse the fitness/
      \textbf{profitiblily} of each foodsource is. This may result in invalid
      results at the end of the algorithm.

    \end{subsubsection}

  \end{subsection}

  \begin{subsection}{Fireflies (Partical Swarm 2.0)}

    Aimed at optimisation problems. In this algorithm firefly possitions
    correstpond to potential solutions and their brightness to their fitness.
    Firefly movement is govenred by \textbf{attractiveness}, which is
    proportional to their brightness and distance from that brightness (inverse
    square??) plus some random movement (mostly usefull where no lights are
    visible).

  \end{subsection}

  \begin{subsection}{Whales}

    This algorithm assumes the target (best solutin) is close to the current
    best possition. At any time (itteration) each whale X tries to get closer
    to \(X*\) (best solution) by \(X(t + 1) = X*(t) - a \times D\). Where
    \(D = |C \times X*(t) - X(t)\), \(C \in [0, 2]{}^{n}\) and \(A \in [-a, a]\)
    for n begin dimention of the solution space \(\mathbb{R}^{n}\) and a
    decreasing with t. Alternatively there is the spiral updating in which
    \(X(t + 1) = X*(t) + e^{bl}\cos(2 \pi l)D'\) where b is a constant and
    \(l \in [0, 1]\) (D' same as D above without C).

  \end{subsection}

  \begin{subsection}{Bats}

    Essentially flying whales??

  \end{subsection}

  \begin{subsection}{Overall Analysis}

    General algorithm:
    \begin{itemize}
      \item Initialise random pop
      \item Compute fitness
      \item If condition satisfied, end
      \item Else compute operation on population and update
      \item Go back to compute fitness
    \end{itemize}

    There are three main variations in the algorithms:
    \begin{itemize}
      \item Learning strategy
      \item Topological structure
            \begin{itemize}
              \item Local neighbourhood
              \item Global neighbourhood
            \end{itemize}
      \item Parameter tuning (can be done with a different or even the same
            natural algorithm, infinite recursion)
    \end{itemize}

  \end{subsection}

\end{section}

\end{document}
